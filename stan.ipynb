{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jimue\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jimue\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\jimue\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from os import path\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "import pylab as py\n",
    "from scipy.sparse import hstack\n",
    "from scipy.sparse import coo_matrix\n",
    "from tqdm import tqdm\n",
    "from scipy import sparse\n",
    "import csv, random, numpy, os, re, scipy, gensim\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import tree\n",
    "from langdetect import detect\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from csv import DictReader\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "# import score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tangina kinopya kolang itong mother fucker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "bodies = pd.read_csv('./Data_Stance/train_bodies.csv')\n",
    "stances = pd.read_csv('./Data_Stance/train_stances.csv')\n",
    "\n",
    "dtset1_bodies = pd.DataFrame(bodies)\n",
    "dtset1_stances = pd.DataFrame(stances)\n",
    "\n",
    "class DataSet():\n",
    "    def __init__(self, path=\"data\"):\n",
    "        self.path = path\n",
    "\n",
    "        self.stances = self.read(stances)\n",
    "        articles = self.read(bodies)\n",
    "        self.articles = dict()\n",
    "\n",
    "        #make the body ID an integer value\n",
    "        for s in self.stances:\n",
    "            s['Body ID'] = int(s['Body ID'])\n",
    "\n",
    "        for article in articles:\n",
    "            self.articles[int(article['Body ID'])] = article['articleBody']\n",
    "\n",
    "        print(\"Total stances: \" + str(len(self.stances)))\n",
    "        print(\"Total bodies: \" + str(len(self.articles)))\n",
    "\n",
    "    def read(self,filename):\n",
    "        rows = []\n",
    "        with open( \"Data_Stance/\" + filename, \"r\", encoding='utf-8') as table:\n",
    "            r = DictReader(table)\n",
    "\n",
    "            for line in r:\n",
    "                rows.append(line)\n",
    "        return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Body ID</th>\n",
       "      <th>articleBody</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>A small meteorite crashed into a wooded area i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>Last week we hinted at what was to come as Ebo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>(NEWSER) – Wonder how long a Quarter Pounder w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>Posting photos of a gun-toting child online, I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>At least 25 suspected Boko Haram insurgents we...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Body ID                                        articleBody\n",
       "0        0  A small meteorite crashed into a wooded area i...\n",
       "1        4  Last week we hinted at what was to come as Ebo...\n",
       "2        5  (NEWSER) – Wonder how long a Quarter Pounder w...\n",
       "3        6  Posting photos of a gun-toting child online, I...\n",
       "4        7  At least 25 suspected Boko Haram insurgents we..."
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.head(dtset1_bodies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Body ID</th>\n",
       "      <th>Stance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Police find mass graves with at least '15 bodi...</td>\n",
       "      <td>712</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hundreds of Palestinians flee floods in Gaza a...</td>\n",
       "      <td>158</td>\n",
       "      <td>agree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Christian Bale passes on role of Steve Jobs, a...</td>\n",
       "      <td>137</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HBO and Apple in Talks for $15/Month Apple TV ...</td>\n",
       "      <td>1034</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Spider burrowed through tourist's stomach and ...</td>\n",
       "      <td>1923</td>\n",
       "      <td>disagree</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Headline  Body ID     Stance\n",
       "0  Police find mass graves with at least '15 bodi...      712  unrelated\n",
       "1  Hundreds of Palestinians flee floods in Gaza a...      158      agree\n",
       "2  Christian Bale passes on role of Steve Jobs, a...      137  unrelated\n",
       "3  HBO and Apple in Talks for $15/Month Apple TV ...     1034  unrelated\n",
       "4  Spider burrowed through tourist's stomach and ...     1923   disagree"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.head(dtset1_stances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset\n",
      "Total stances: 49972\n",
      "Total bodies: 1683\n"
     ]
    }
   ],
   "source": [
    "dataset = DataSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Headline': \"Spider burrowed through tourist's stomach and up into his chest\",\n",
       " 'Body ID': 1923,\n",
       " 'Stance': 'disagree'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.stances[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ids(file,base):\n",
    "    ids = []\n",
    "    with open(base+\"/\"+file,\"r\") as f:\n",
    "        for line in f:\n",
    "            ids.append(int(line))\n",
    "        return ids\n",
    "    \n",
    "def split(dataset, base=\"splits\"):\n",
    "    if not (os.path.exists(base+\"/\"+\"training_ids.txt\")\n",
    "            and os.path.exists(base+\"/\"+\"dev_ids.txt\") and os.path.exists(base+\"/\"+\"test_ids.txt\")):\n",
    "        raise Exception(\"There is an error and the dataset reader cannot find the \"\n",
    "                        \"{training_ids|test_ids|dev_ids}.txt file. Please make sure your python paths \"\n",
    "                        \"are configured correctly\")\n",
    "\n",
    "    training_ids = read_ids(\"training_ids.txt\",base)\n",
    "    dev_ids = read_ids(\"dev_ids.txt\",base)\n",
    "    test_ids = read_ids(\"test_ids.txt\",base)\n",
    "\n",
    "    #return the stances that meet these criteria\n",
    "    training_stances = []\n",
    "    dev_stances = []\n",
    "    test_stances = []\n",
    "\n",
    "    for stance in dataset.stances:\n",
    "        if stance['Body ID'] in training_ids:\n",
    "            training_stances.append(stance)\n",
    "        elif stance['Body ID'] in dev_ids:\n",
    "            dev_stances.append(stance)\n",
    "        elif stance['Body ID'] in test_ids:\n",
    "            test_stances.append(stance)\n",
    "\n",
    "\n",
    "    return {\"training\":training_stances, \"dev\":dev_stances, \"test\": test_stances}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: 40106\n",
      "Dev size: 4835\n",
      "Test data: 5031\n"
     ]
    }
   ],
   "source": [
    "data_splits = split(dataset)\n",
    "\n",
    "training_data = data_splits['training']\n",
    "dev_data = data_splits['dev']\n",
    "test_data = data_splits['test']\n",
    "\n",
    "N = int(len(training_data) * 1.0)\n",
    "training_data = training_data[:N]\n",
    "\n",
    "print(\"Training size:\", len(training_data))\n",
    "print(\"Dev size:\", len(dev_data))\n",
    "print(\"Test data:\", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1683"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bodies(data):\n",
    "    bodies = []\n",
    "    for i in range(len(data)):\n",
    "        bodies.append(dataset.articles[data[i]['Body ID']])\n",
    "    return bodies\n",
    "\n",
    "\n",
    "def get_headlines(data):\n",
    "    headlines = []\n",
    "    for i in range(len(data)):\n",
    "        headlines.append(data[i]['Headline'])\n",
    "    return headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40106\n",
      "40106\n",
      "4835\n",
      "4835\n",
      "5031\n",
      "5031\n"
     ]
    }
   ],
   "source": [
    "training_bodies = get_bodies(training_data)\n",
    "training_headlines = get_headlines(training_data)\n",
    "dev_bodies = get_bodies(dev_data)\n",
    "dev_headlines = get_headlines(dev_data)\n",
    "test_bodies = get_bodies(test_data)\n",
    "test_headlines = get_headlines(test_data)\n",
    "\n",
    "print(len(training_bodies))\n",
    "print(len(training_headlines))\n",
    "print(len(dev_bodies))\n",
    "print(len(dev_headlines))\n",
    "print(len(test_bodies))\n",
    "print(len(test_headlines))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a6b989f8aea76d281d40c171688fb68eaa44ba401600b93bc7d7cc3f2b5fca20"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
